{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc356ae85af4b5491760a9f6d24d330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"/teamspace/studios/this_studio/llm-models/gemma1_2b_model_transformer\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "# Load the model with device mapping\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:633: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: The future of AI is uncertain, but one thing is clear: AI will have a profound impact on society.\n",
      "\n",
      "**a)** Discuss the potential benefits and risks of artificial intelligence.\n",
      "**b)** Explain how AI can be used to solve complex\n"
     ]
    }
   ],
   "source": [
    "# Sample text input\n",
    "input_text = \"The future of AI is\"\n",
    "\n",
    "# Tokenize the input\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# Generate predictions\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_length=50,  # Maximum length of the generated text\n",
    "    num_beams=5,  # Number of beams for beam search (optional)\n",
    "    no_repeat_ngram_size=2,  # Avoid repeating n-grams (optional)\n",
    "    early_stopping=True,  # Stop early when enough beams are found\n",
    "    temperature=0.7,  # Sampling temperature (lower is more deterministic)\n",
    "    top_k=50,  # Limits sampling to the top-k tokens (optional)\n",
    "    top_p=0.9,  # Nucleus sampling threshold (optional)\n",
    ")\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated text:\", generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks.python.genai import converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CKPT = '/teamspace/studios/this_studio/llm-models/gemma1_2b_model_transformer'\n",
    "CKPT_FORMAT='safetensors'\n",
    "MODEL_TYPE='GEMMA_2B'\n",
    "combine_file_only=False\n",
    "BACKEND = \"gpu\"\n",
    "VOCAB_MODEL_FILE = '/teamspace/studios/this_studio/llm-models/gemma1_2b_model_transformer'\n",
    "OUTPUT_DIR = '/teamspace/studios/this_studio/converted-bin-models/gemma1-quantized/'\n",
    "OUTPUT_TFLITE_FILE = '/teamspace/studios/this_studio/converted-bin-models/gemma1-quantized/gemma1_2b_4bq.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734899671.083876   27597 model_ckpt_util.cc:337] Inserting empty buffer to TfLite.\n"
     ]
    }
   ],
   "source": [
    "config = converter.ConversionConfig(\n",
    "  input_ckpt=INPUT_CKPT,\n",
    "  ckpt_format=CKPT_FORMAT,\n",
    "  model_type=MODEL_TYPE,\n",
    "  backend=BACKEND,\n",
    "  output_dir=OUTPUT_DIR,\n",
    "  combine_file_only=combine_file_only,\n",
    "  vocab_model_file=VOCAB_MODEL_FILE,\n",
    "  output_tflite_file=OUTPUT_TFLITE_FILE,\n",
    "  attention_quant_bits=4,\n",
    "  feedforward_quant_bits=4,\n",
    "  embedding_quant_bits=8,\n",
    ")\n",
    "\n",
    "converter.convert_checkpoint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
