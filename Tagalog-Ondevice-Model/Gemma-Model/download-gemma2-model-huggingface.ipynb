{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loading model get access from the hugging face and load the token\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv(override=True)\n",
    "hf_access_token = os.environ[\"hf_access_token\"]\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=hf_access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Gemma 2 2B IT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684268455c194834bcf0603e7195e3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2-2b-it\",device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos>Write me a poem about Machine Learning.\n",
      "\n",
      "A tapestry of data, woven tight,\n",
      "A million threads of knowledge, shining bright.\n",
      "Algorithms dance, a rhythmic sway,\n",
      "Learning patterns, come\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Write me a poem about Machine Learning.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**input_ids, max_new_tokens=32)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target directory where you want to move the model\n",
    "target_path = os.path.expanduser(\"~/llm-models/gemma2_2b_model_transformer\")\n",
    "\n",
    "# Create the target directory if it does not exist\n",
    "os.makedirs(target_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer and model to a directory\n",
    "tokenizer.save_pretrained(target_path)\n",
    "model.save_pretrained(target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to BIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks.python.genai import converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CKPT = '/teamspace/studios/this_studio/llm-models/gemma2_2b_model_transformer'\n",
    "CKPT_FORMAT='safetensors'\n",
    "MODEL_TYPE='GEMMA_2B'\n",
    "combine_file_only=False\n",
    "BACKEND = \"gpu\"\n",
    "VOCAB_MODEL_FILE = '/teamspace/studios/this_studio/llm-models/gemma2_2b_model_transformer'\n",
    "OUTPUT_DIR = '/teamspace/studios/this_studio/converted-bin-models/gemma2/'\n",
    "OUTPUT_TFLITE_FILE = '/teamspace/studios/this_studio/converted-bin-models/gemma2/gemma2.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = converter.ConversionConfig(\n",
    "  input_ckpt=INPUT_CKPT,\n",
    "  ckpt_format=CKPT_FORMAT,\n",
    "  model_type=MODEL_TYPE,\n",
    "  backend=BACKEND,\n",
    "  output_dir=OUTPUT_DIR,\n",
    "  combine_file_only=combine_file_only,\n",
    "  vocab_model_file=VOCAB_MODEL_FILE,\n",
    "  output_tflite_file=OUTPUT_TFLITE_FILE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2024-12-23 19:10:57,011:jax._src.xla_bridge:969: An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "WARNING:jax._src.xla_bridge:An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1734981180.633455    2171 model_ckpt_util.cc:337] Inserting empty buffer to TfLite.\n"
     ]
    }
   ],
   "source": [
    "converter.convert_checkpoint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_CKPT = '/teamspace/studios/this_studio/llm-models/gemma2_2b_model_transformer'\n",
    "CKPT_FORMAT='safetensors'\n",
    "MODEL_TYPE='GEMMA_2B'\n",
    "combine_file_only=False\n",
    "BACKEND = \"gpu\"\n",
    "VOCAB_MODEL_FILE = '/teamspace/studios/this_studio/llm-models/gemma2_2b_model_transformer'\n",
    "OUTPUT_DIR = '/teamspace/studios/this_studio/converted-bin-models/gemma2-quantized/'\n",
    "OUTPUT_TFLITE_FILE = '/teamspace/studios/this_studio/converted-bin-models/gemma2-quantized/gemma2_2b_4bq.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = converter.ConversionConfig(\n",
    "  input_ckpt=INPUT_CKPT,\n",
    "  ckpt_format=CKPT_FORMAT,\n",
    "  model_type=MODEL_TYPE,\n",
    "  backend=BACKEND,\n",
    "  output_dir=OUTPUT_DIR,\n",
    "  combine_file_only=combine_file_only,\n",
    "  vocab_model_file=VOCAB_MODEL_FILE,\n",
    "  output_tflite_file=OUTPUT_TFLITE_FILE,\n",
    "  attention_quant_bits=4,\n",
    "  feedforward_quant_bits=4,\n",
    "  embedding_quant_bits=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1734982389.981357    2171 model_ckpt_util.cc:337] Inserting empty buffer to TfLite.\n"
     ]
    }
   ],
   "source": [
    "converter.convert_checkpoint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
