{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --quiet --upgrade \\\n",
    "    pip \\\n",
    "    python-dotenv \\\n",
    "    datasets \\\n",
    "    accelerate \\\n",
    "    peft \\\n",
    "    bitsandbytes \\\n",
    "    transformers \\\n",
    "    trl \\\n",
    "    sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    TrainingArguments,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./gpt2_tagalog\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pad token to be the same as the eos token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and analyze the data for finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'chosen', 'rejected', 'prompt', 'question'],\n",
       "    num_rows: 44245\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Dataset\n",
    "dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'chosen', 'rejected', 'prompt', 'question'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filter the dataset to 1000 examples\n",
    "dataset = dataset.shuffle(seed=42).select(range(100))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'ultrachat',\n",
       " 'chosen': [{'content': 'Can you provide examples of how different cultures approach grief and mourning, and how this has evolved over time?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Of course, I\\'ll do my best to provide you with accurate and helpful information.\\n\\nGrief and mourning are universal human experiences, and different cultures have developed unique ways of coping with loss and honoring the deceased. These practices have evolved over time, reflecting changes in societal values, beliefs, and traditions. Here are some examples:\\n\\n1. Ancient Egypt: In ancient Egyptian culture, mourning was a complex and elaborate process that lasted for 70 days. During this time, the family of the deceased would observe a period of mourning, known as \"the sorrows of the heart,\" which involved abstaining from certain foods, wearing plain clothes, and covering their faces with dust and ashes. After 70 days, the family would hold a feast to celebrate the deceased\\'s transition to the afterlife.\\n2. Hinduism: In Hindu culture, the mourning period typically lasts for 13 days, during which time the family observes rituals and ceremonies to help the deceased\\'s soul transition to the afterlife. The family members are expected to follow certain customs, such as wearing simple clothes, avoiding certain foods, and not participating in social events. On the 13th day, a ritual called \"Shraddha\" is performed, which involves offering food and water to the deceased\\'s soul.\\n3. Buddhism: In Buddhist culture, mourning is seen as an opportunity for spiritual growth and reflection. The family of the deceased typically observes a period of mourning that lasts for 49 days, during which time they are encouraged to focus on spiritual practices such as meditation, chanting, and offering food and incense to the monks. On the 49th day, a ceremony called \"Bardo\" is held to help the deceased\\'s soul transition to the next life.\\n4. Victorian England: During the Victorian era, mourning was a highly ritualized and codified practice that lasted for several years. The family of the deceased would wear black clothing and jewelry, and observe strict social norms, such as avoiding social events and not participating in certain activities. The length and intensity of the mourning period varied depending on the social status of the deceased and the relationship of the mourner to the deceased.\\n5. Modern-day Japan: In contemporary Japanese culture, mourning is typically a private and individualized experience. The family of the deceased may observe a period of mourning that lasts for several days or weeks, during which time they may visit the gravesite, offer food and incense, and engage in spiritual practices such as Zen Buddhism. The Japanese also have a tradition of \"condolence money,\" where friends and family members give money to the bereaved family to help offset the costs of the funeral and to show their respects.\\n\\nThese examples illustrate how different cultures have developed unique ways of coping with loss and honoring the deceased, and how these practices have evolved over time to reflect changes in societal values and beliefs. It\\'s important to note that each culture has its own unique traditions and customs surrounding grief and mourning, and it\\'s essential to respect and appreciate these differences.',\n",
       "   'role': 'assistant'}],\n",
       " 'rejected': [{'content': 'Can you provide examples of how different cultures approach grief and mourning, and how this has evolved over time?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Grief and mourning practices have varied greatly across cultures and through time. For example, in some cultures mourners will shave their head to signify their grief, while in other cultures people may cut their finger nails as a sign of mourning. In some cultures, the period of mourning is strictly regulated and controlled, while in other cultures, individuals has the freedom to express their grief in whatever way they choose. Over time, some cultural practices have developed to help support the grieving process, such as sending food and other gifts to the mourning person.',\n",
       "   'role': 'assistant'}],\n",
       " 'prompt': 'Can you provide examples of how different cultures approach grief and mourning, and how this has evolved over time?',\n",
       " 'question': 'Can you provide examples of how different cultures approach grief and mourning, and how this has evolved over time?'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_chat_template(row):\n",
    "    # Formatting for the chosen response\n",
    "    chosen_template = \"\"\"\n",
    "    <|user|>\n",
    "    {user_message}\n",
    "    <|assistant|>\n",
    "    {assistant_message}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    # Formatting for the rejected response\n",
    "    rejected_template = \"\"\"\n",
    "    <|user|>\n",
    "    {user_message}\n",
    "    <|assistant|>\n",
    "    {assistant_message}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    formatted_chosen = chosen_template.format(\n",
    "        user_message=row[\"chosen\"][0][\"content\"], \n",
    "        assistant_message=row[\"chosen\"][1][\"content\"]\n",
    "    )\n",
    "\n",
    "    formatted_rejected = rejected_template.format(\n",
    "        user_message=row[\"rejected\"][0][\"content\"], \n",
    "        assistant_message=row[\"rejected\"][1][\"content\"]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"chosen\": formatted_chosen,\n",
    "        \"rejected\": formatted_rejected\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method PreTrainedTokenizerBase.apply_chat_template of GPT2TokenizerFast(name_or_path='./gpt2_tagalog', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the formatting function\n",
    "formatted_dataset = dataset.map(format_chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'ultrachat',\n",
       " 'chosen': '<|user|>\\n    Can you provide examples of how different cultures approach grief and mourning, and how this has evolved over time?\\n    <|assistant|>\\n    Of course, I\\'ll do my best to provide you with accurate and helpful information.\\n\\nGrief and mourning are universal human experiences, and different cultures have developed unique ways of coping with loss and honoring the deceased. These practices have evolved over time, reflecting changes in societal values, beliefs, and traditions. Here are some examples:\\n\\n1. Ancient Egypt: In ancient Egyptian culture, mourning was a complex and elaborate process that lasted for 70 days. During this time, the family of the deceased would observe a period of mourning, known as \"the sorrows of the heart,\" which involved abstaining from certain foods, wearing plain clothes, and covering their faces with dust and ashes. After 70 days, the family would hold a feast to celebrate the deceased\\'s transition to the afterlife.\\n2. Hinduism: In Hindu culture, the mourning period typically lasts for 13 days, during which time the family observes rituals and ceremonies to help the deceased\\'s soul transition to the afterlife. The family members are expected to follow certain customs, such as wearing simple clothes, avoiding certain foods, and not participating in social events. On the 13th day, a ritual called \"Shraddha\" is performed, which involves offering food and water to the deceased\\'s soul.\\n3. Buddhism: In Buddhist culture, mourning is seen as an opportunity for spiritual growth and reflection. The family of the deceased typically observes a period of mourning that lasts for 49 days, during which time they are encouraged to focus on spiritual practices such as meditation, chanting, and offering food and incense to the monks. On the 49th day, a ceremony called \"Bardo\" is held to help the deceased\\'s soul transition to the next life.\\n4. Victorian England: During the Victorian era, mourning was a highly ritualized and codified practice that lasted for several years. The family of the deceased would wear black clothing and jewelry, and observe strict social norms, such as avoiding social events and not participating in certain activities. The length and intensity of the mourning period varied depending on the social status of the deceased and the relationship of the mourner to the deceased.\\n5. Modern-day Japan: In contemporary Japanese culture, mourning is typically a private and individualized experience. The family of the deceased may observe a period of mourning that lasts for several days or weeks, during which time they may visit the gravesite, offer food and incense, and engage in spiritual practices such as Zen Buddhism. The Japanese also have a tradition of \"condolence money,\" where friends and family members give money to the bereaved family to help offset the costs of the funeral and to show their respects.\\n\\nThese examples illustrate how different cultures have developed unique ways of coping with loss and honoring the deceased, and how these practices have evolved over time to reflect changes in societal values and beliefs. It\\'s important to note that each culture has its own unique traditions and customs surrounding grief and mourning, and it\\'s essential to respect and appreciate these differences.',\n",
       " 'rejected': '<|user|>\\n    Can you provide examples of how different cultures approach grief and mourning, and how this has evolved over time?\\n    <|assistant|>\\n    Grief and mourning practices have varied greatly across cultures and through time. For example, in some cultures mourners will shave their head to signify their grief, while in other cultures people may cut their finger nails as a sign of mourning. In some cultures, the period of mourning is strictly regulated and controlled, while in other cultures, individuals has the freedom to express their grief in whatever way they choose. Over time, some cultural practices have developed to help support the grieving process, such as sending food and other gifts to the mourning person.',\n",
       " 'prompt': 'Can you provide examples of how different cultures approach grief and mourning, and how this has evolved over time?',\n",
       " 'question': 'Can you provide examples of how different cultures approach grief and mourning, and how this has evolved over time?'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'chosen', 'rejected', 'prompt', 'question'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'chosen', 'rejected', 'prompt', 'question'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split the data for train and test\n",
    "formatted_dataset = formatted_dataset.train_test_split(test_size=0.10)\n",
    "formatted_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lm_head']\n"
     ]
    }
   ],
   "source": [
    "def find_linear_layers(model):\n",
    "    # Initialize a list to store the names of all linear layers\n",
    "    linear_layers = []\n",
    "    \n",
    "    # Iterate over all named modules in the model\n",
    "    for name, module in model.named_modules():\n",
    "        # Check if the module is of type Linear (from nn.Module)\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # Append the name of the layer to the list\n",
    "            linear_layers.append(name)\n",
    "    \n",
    "    # Return the list of linear layer names\n",
    "    return linear_layers\n",
    "\n",
    "# Assuming 'model' is your GPT-2 model\n",
    "linear_layer_names = find_linear_layers(model)\n",
    "print(linear_layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default configs\n",
    "attn_implementation = \"eager\"\n",
    "torch_dtype = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['lm_head']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Setup ORPO Configuration\n",
    "orpo_args = ORPOConfig(\n",
    "    learning_rate=8e-6,\n",
    "    beta=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    save_steps=5,\n",
    "    output_dir=\"./tagalog-results\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:500: UserWarning: Model with `tie_word_embeddings=True` and the tied_target_modules=['lm_head'] are part of the adapter. This can lead to complications, for example when merging the adapter or converting your model to formats other than safetensors. See for example https://github.com/huggingface/peft/issues/2018.\n",
      "  warnings.warn(\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/trl/trainer/orpo_trainer.py:276: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f060348345b49a8a7bd6b3662256875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3722dee49634b64bb4e2436781b52ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6085b3f5c191415daf99aad7158db122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e0d89650e1845e2a247cfd36d0d4c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a57bfb43154b2eb36f3ac77960b735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c2126090914be9865e0eacec5dc607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Setup ORPO Trainer\n",
    "trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_args,\n",
    "    train_dataset=formatted_dataset[\"train\"],\n",
    "    eval_dataset=formatted_dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/22 00:17, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Nll Loss</th>\n",
       "      <th>Log Odds Ratio</th>\n",
       "      <th>Log Odds Chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.485100</td>\n",
       "      <td>8.008080</td>\n",
       "      <td>1.010800</td>\n",
       "      <td>9.894000</td>\n",
       "      <td>4.947000</td>\n",
       "      <td>-0.778745</td>\n",
       "      <td>-0.733009</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.045736</td>\n",
       "      <td>-7.330093</td>\n",
       "      <td>-7.787448</td>\n",
       "      <td>-3.016872</td>\n",
       "      <td>-3.016319</td>\n",
       "      <td>7.897358</td>\n",
       "      <td>-1.107213</td>\n",
       "      <td>-0.458247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.268300</td>\n",
       "      <td>8.007823</td>\n",
       "      <td>1.007700</td>\n",
       "      <td>9.923000</td>\n",
       "      <td>4.962000</td>\n",
       "      <td>-0.778718</td>\n",
       "      <td>-0.732982</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.045736</td>\n",
       "      <td>-7.329820</td>\n",
       "      <td>-7.787179</td>\n",
       "      <td>-3.017571</td>\n",
       "      <td>-3.017027</td>\n",
       "      <td>7.897102</td>\n",
       "      <td>-1.107210</td>\n",
       "      <td>-0.458250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>5.841800</td>\n",
       "      <td>8.007494</td>\n",
       "      <td>1.010100</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>4.950000</td>\n",
       "      <td>-0.778683</td>\n",
       "      <td>-0.732948</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.045735</td>\n",
       "      <td>-7.329480</td>\n",
       "      <td>-7.786832</td>\n",
       "      <td>-3.018563</td>\n",
       "      <td>-3.018042</td>\n",
       "      <td>7.896774</td>\n",
       "      <td>-1.107198</td>\n",
       "      <td>-0.458244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.152300</td>\n",
       "      <td>8.007330</td>\n",
       "      <td>1.008300</td>\n",
       "      <td>9.918000</td>\n",
       "      <td>4.959000</td>\n",
       "      <td>-0.778666</td>\n",
       "      <td>-0.732931</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.045735</td>\n",
       "      <td>-7.329309</td>\n",
       "      <td>-7.786659</td>\n",
       "      <td>-3.019150</td>\n",
       "      <td>-3.018640</td>\n",
       "      <td>7.896610</td>\n",
       "      <td>-1.107192</td>\n",
       "      <td>-0.458241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
      "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=22, training_loss=6.2801573059775615, metrics={'train_runtime': 19.1045, 'train_samples_per_second': 4.711, 'train_steps_per_second': 1.152, 'total_flos': 0.0, 'train_loss': 6.2801573059775615, 'epoch': 0.9777777777777777})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
